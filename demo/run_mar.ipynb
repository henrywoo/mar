{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "355UKMUQJxFd"
   },
   "source": [
    "# Autoregressive Image Generation without Vector Quantization\n",
    "\n",
    "This notebook samples from pre-trained MAR+DiffLoss models, which are class-conditional masked autoregressive models trained on ImageNet to model continuous latent tokens. MAR+DiffLoss achieves state-of-the-art performance on the ImageNet benchmarks. A large portion of this demo is adopted from [DiT](https://github.com/facebookresearch/DiT).\n",
    "\n",
    "[Paper](https://arxiv.org/abs/2406.11838) | [GitHub](https://github.com/LTH14/mar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zJlgLkSaKn7u"
   },
   "source": [
    "# 1. Setup\n",
    "\n",
    "We recommend using GPUs (Runtime > Change runtime type > Hardware accelerator > GPU). Run this cell to clone the MAR GitHub repo and setup PyTorch. You only have to run this once."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "y9RUzQUWhZEV",
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-17T14:40:28.258025Z",
     "start_time": "2024-08-17T14:40:14.278324Z"
    }
   },
   "source": [
    "import os\n",
    "\n",
    "#!git clone https://github.com/LTH14/mar.git\n",
    "import os\n",
    "#os.chdir('mar')\n",
    "#os.environ['PYTHONPATH'] = '/env/python:/content/mar'\n",
    "#!pip install timm==0.9.12\n",
    "# MAR imports:\n",
    "import torch\n",
    "import numpy as np\n",
    "from models import mar\n",
    "from models.vae import AutoencoderKL\n",
    "from torchvision.utils import save_image\n",
    "from util import download\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "torch.set_grad_enabled(False)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if device == \"cpu\":\n",
    "    print(\"GPU not found. Using CPU instead.\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wukong/miniconda3/envs/genaibook/lib/python3.10/site-packages/beartype/_util/error/utilerrwarn.py:67: BeartypeModuleUnimportableWarning: Ignoring module \"onnx\" importation exception:\n",
      "    ImportError: cannot import name 'builder' from 'google.protobuf.internal' (/home/wukong/miniconda3/envs/genaibook/lib/python3.10/site-packages/google/protobuf/internal/__init__.py)\n",
      "  warn(message, cls)\n",
      "/home/wukong/miniconda3/envs/genaibook/lib/python3.10/site-packages/beartype/_util/error/utilerrwarn.py:67: BeartypeModuleUnimportableWarning: Ignoring module \"onnx\" importation exception:\n",
      "    ImportError: cannot import name 'builder' from 'google.protobuf.internal' (/home/wukong/miniconda3/envs/genaibook/lib/python3.10/site-packages/google/protobuf/internal/__init__.py)\n",
      "  warn(message, cls)\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AXpziRkoOvV9"
   },
   "source": [
    "# Download Pre-trained VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "EWG-WNimO59K"
   },
   "outputs": [],
   "source": [
    "download.download_pretrained_vae()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "pe8NYbWBsvLl"
   },
   "source": [
    "# 2. Load and download pre-trained MAR models"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "x0EKkB_ssvLl",
    "ExecuteTime": {
     "end_time": "2024-08-17T14:43:35.897755Z",
     "start_time": "2024-08-17T14:43:30.163672Z"
    }
   },
   "source": [
    "model_type = \"mar_huge\" #@param [\"mar_base\", \"mar_large\", \"mar_huge\"]\n",
    "num_sampling_steps_diffloss = 100 #@param {type:\"slider\", min:1, max:1000, step:1}\n",
    "if model_type == \"mar_base\":\n",
    "  #download.download_pretrained_marb(overwrite=False)\n",
    "  diffloss_d = 6\n",
    "  diffloss_w = 1024\n",
    "elif model_type == \"mar_large\":\n",
    "  #download.download_pretrained_marl(overwrite=False)\n",
    "  diffloss_d = 8\n",
    "  diffloss_w = 1280\n",
    "elif model_type == \"mar_huge\":\n",
    "  #download.download_pretrained_marh(overwrite=False)\n",
    "  diffloss_d = 12\n",
    "  diffloss_w = 1536\n",
    "else:\n",
    "  raise NotImplementedError\n",
    "model = mar.__dict__[model_type](\n",
    "  buffer_size=64,\n",
    "  diffloss_d=diffloss_d,\n",
    "  diffloss_w=diffloss_w,\n",
    "  num_sampling_steps=str(num_sampling_steps_diffloss)\n",
    ").to(device)\n",
    "state_dict = torch.load(\"pretrained_models/mar/{}/checkpoint-last.pth\".format(model_type))[\"model_ema\"]\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval() # important!\n",
    "vae = AutoencoderKL(embed_dim=16, ch_mult=(1, 1, 2, 2, 4), ckpt_path=\"pretrained_models/vae/kl16.ckpt\").cuda().eval()"
   ],
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "PytorchStreamReader failed reading zip archive: failed finding central directory",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 23\u001B[0m\n\u001B[1;32m     16\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mNotImplementedError\u001B[39;00m\n\u001B[1;32m     17\u001B[0m model \u001B[38;5;241m=\u001B[39m mar\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__dict__\u001B[39m[model_type](\n\u001B[1;32m     18\u001B[0m   buffer_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m64\u001B[39m,\n\u001B[1;32m     19\u001B[0m   diffloss_d\u001B[38;5;241m=\u001B[39mdiffloss_d,\n\u001B[1;32m     20\u001B[0m   diffloss_w\u001B[38;5;241m=\u001B[39mdiffloss_w,\n\u001B[1;32m     21\u001B[0m   num_sampling_steps\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mstr\u001B[39m(num_sampling_steps_diffloss)\n\u001B[1;32m     22\u001B[0m )\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m---> 23\u001B[0m state_dict \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mpretrained_models/mar/\u001B[39;49m\u001B[38;5;132;43;01m{}\u001B[39;49;00m\u001B[38;5;124;43m/checkpoint-last.pth\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mformat\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_type\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel_ema\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m     24\u001B[0m model\u001B[38;5;241m.\u001B[39mload_state_dict(state_dict)\n\u001B[1;32m     25\u001B[0m model\u001B[38;5;241m.\u001B[39meval() \u001B[38;5;66;03m# important!\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/genaibook/lib/python3.10/site-packages/torch/serialization.py:1004\u001B[0m, in \u001B[0;36mload\u001B[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001B[0m\n\u001B[1;32m   1002\u001B[0m orig_position \u001B[38;5;241m=\u001B[39m opened_file\u001B[38;5;241m.\u001B[39mtell()\n\u001B[1;32m   1003\u001B[0m overall_storage \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m-> 1004\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[43m_open_zipfile_reader\u001B[49m\u001B[43m(\u001B[49m\u001B[43mopened_file\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m opened_zipfile:\n\u001B[1;32m   1005\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m _is_torchscript_zip(opened_zipfile):\n\u001B[1;32m   1006\u001B[0m         warnings\u001B[38;5;241m.\u001B[39mwarn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtorch.load\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m received a zip file that looks like a TorchScript archive\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1007\u001B[0m                       \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m dispatching to \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtorch.jit.load\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m (call \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtorch.jit.load\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m directly to\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1008\u001B[0m                       \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m silence this warning)\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;167;01mUserWarning\u001B[39;00m)\n",
      "File \u001B[0;32m~/miniconda3/envs/genaibook/lib/python3.10/site-packages/torch/serialization.py:456\u001B[0m, in \u001B[0;36m_open_zipfile_reader.__init__\u001B[0;34m(self, name_or_buffer)\u001B[0m\n\u001B[1;32m    455\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, name_or_buffer) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 456\u001B[0m     \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_C\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mPyTorchFileReader\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname_or_buffer\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[0;31mRuntimeError\u001B[0m: PytorchStreamReader failed reading zip archive: failed finding central directory"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5JTNyzNZKb9E"
   },
   "source": [
    "# 3. Sample from Pre-trained MAR Models\n",
    "\n",
    "You can customize several sampling options. For the full list of ImageNet classes, [check out this](https://deeplearning.cms.waikato.ac.nz/user-guide/class-maps/IMAGENET/)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 553
    },
    "id": "-Hw7B5h4Kk4p",
    "outputId": "4948292b-e1d2-4ce9-de75-e58cdd74f0c4",
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-08-17T14:43:42.626729Z"
    }
   },
   "source": [
    "# Set user inputs:\n",
    "seed = 0 #@param {type:\"number\"}\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "num_ar_steps = 64 #@param {type:\"slider\", min:1, max:256, step:1}\n",
    "cfg_scale = 4 #@param {type:\"slider\", min:1, max:10, step:0.1}\n",
    "cfg_schedule = \"constant\" #@param [\"linear\", \"constant\"]\n",
    "temperature = 1.0 #@param {type:\"slider\", min:0.9, max:1.1, step:0.01}\n",
    "class_labels = 207, 360, 388, 113, 355, 980, 323, 979 #@param {type:\"raw\"}\n",
    "samples_per_row = 4 #@param {type:\"number\"}\n",
    "\n",
    "with torch.cuda.amp.autocast():\n",
    "  sampled_tokens = model.sample_tokens(\n",
    "      bsz=len(class_labels), num_iter=num_ar_steps,\n",
    "      cfg=cfg_scale, cfg_schedule=cfg_schedule,\n",
    "      labels=torch.Tensor(class_labels).long().cuda(),\n",
    "      temperature=temperature, progress=True)\n",
    "  sampled_images = vae.decode(sampled_tokens / 0.2325)\n",
    "\n",
    "# Save and display images:\n",
    "save_image(sampled_images, \"sample.png\", nrow=int(samples_per_row), normalize=True, value_range=(-1, 1))\n",
    "samples = Image.open(\"sample.png\")\n",
    "display(samples)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 57/64 [00:17<00:01,  4.45it/s]"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
